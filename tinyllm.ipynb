{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyFou/testColab/blob/main/tinyllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ful9_femXKTv"
      },
      "source": [
        "Source du tuto\n",
        "https://levelup.gitconnected.com/building-a-perfect-million-parameter-llm-like-chatgpt-in-python-3b16e26b4139\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation du fichier jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8RxLchItBQ9",
        "outputId": "516fa3df-38e0-48cb-d983-facc84232fd8"
      },
      "outputs": [],
      "source": [
        "# faire un ctrl C en fonction de la taille souhaité du jdd\n",
        "#!wget https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl\n",
        "\n",
        "import os,urllib\n",
        "url = 'https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl'\n",
        "filename = './sft_data_en.jsonl'\n",
        "if not os.path.isfile(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "def remove_last_line(filepath):\n",
        "    \"\"\"Removes the last line of a large file efficiently.\n",
        "\n",
        "    Args:\n",
        "      filepath: The path to the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb+') as f:\n",
        "            f.seek(-2, os.SEEK_END)  # Go to the second-to-last byte\n",
        "            while f.read(1) != b'\\n':\n",
        "                f.seek(-2, os.SEEK_CUR)  # Back up two bytes\n",
        "                if f.tell() == 0:\n",
        "                    # Handle the case where there's only one line or no newline chars.\n",
        "                    f.truncate(0)\n",
        "                    return\n",
        "            f.truncate()\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing last line from {filepath}: {e}\")\n",
        "        return\n",
        "\n",
        "remove_last_line(\"sft_data_en.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBmPTblYtEvG"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "!sed -i '$d' sft_data_en.jsonl.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "file_path = \"sft_data_en.jsonl\" # Downloaded filepath (23 GB RAM REQUIRED!!!!!!!!!!!!!!!!!!!!!)\n",
        "\n",
        "# Read the JSONL file with tqdm progress bar\n",
        "data = []\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    # Iterate through each line of the JSONL file\n",
        "    for line in tqdm(file, desc=\"Loading JSONL file\"):\n",
        "        # Parse each line as a JSON object and append to data\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKIpX2vWuPdP",
        "outputId": "7b91832f-496f-4a02-da62-842abaab0e45"
      },
      "outputs": [],
      "source": [
        "\n",
        "##### retravaille le fichier de données : ligne de 512 caratère, supression (en grande partie) des lignes avec caratère asiatique\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "file_path = \"sft_data_en.jsonl\" \n",
        "\n",
        "# Define the output file name\n",
        "output_file = \"petrain_data.jsonl\"\n",
        "\n",
        "lenAfterFilter = 0\n",
        "# Write data to JSONL format\n",
        "asian_char_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3040-\\u30FF\\uAC00-\\uD7AF]')\n",
        "with open(file_path, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for line in tqdm(infile, desc=\"Filter JSONL file\"):\n",
        "        item = json.loads(line)\n",
        "        io_length = len(item['input']) + len(item['output'])\n",
        "        if io_length < 512 and not asian_char_pattern.search(line):\n",
        "            json.dump({\"text\": f\"{item['input']}\\n{item['output']}\"}, outfile, ensure_ascii=False)\n",
        "            outfile.write(\"\\n\")  # Newline for each JSONL entry\n",
        "            lenAfterFilter+=1\n",
        "print(f\"New JSONL file saved as {output_file}\")\n",
        "print(lenAfterFilter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation du data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PretrainDataset(Dataset):\n",
        "    \"\"\"Dataset for pretraining.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Initializes the PretrainDataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the JSONL data file.\n",
        "            tokenizer: The tokenizer to use.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = self.load_data(data_path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        \"\"\"\n",
        "        Loads data from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the JSONL file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of samples loaded from the file.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):  # enumerate starts at 1 for line_num\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input (X), target (Y), and loss mask.\n",
        "        \"\"\"\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        # Construct the input text, including BOS and EOS tokens.\n",
        "        text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',  # Pad to max_length\n",
        "            truncation=True,  # Truncate to max_length\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "        input_ids = encoding.input_ids.squeeze()  # Remove extra dimension\n",
        "        loss_mask = (input_ids != self.tokenizer.pad_token_id)  # Create loss mask (ignore padding)\n",
        "\n",
        "        # Create input (X) and target (Y) tensors, shifting by one position.\n",
        "        X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
        "        Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
        "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n",
        "        return X, Y, loss_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for our Transformer model\n",
        "model_config = {\n",
        "    \"vocab_size\": 6400,       # Size of the vocabulary\n",
        "    \"dim\": 512,               # Dimensionality of the embeddings and hidden states\n",
        "    \"n_heads\": 8,             # Number of attention heads\n",
        "    \"n_kv_heads\": 2,          # Number of key-value heads (as specified in the LMConfig)\n",
        "    \"norm_eps\": 1e-5,         # Epsilon for RMSNorm\n",
        "    \"dropout\": 0.0,           # Dropout probability\n",
        "    \"max_seq_len\": 1024,      # Maximum sequence length\n",
        "    \"rope_theta\": 10000.0,    # Theta parameter for RoPE\n",
        "    \"multiple_of\": 64,        # Used for hidden dimension calculation in FFN\n",
        "    \"hidden_dim\": None,       # Hidden dimension of the FFN (calculated if None)\n",
        "    \"n_layers\": 8,            # Number of Transformer blocks\n",
        "    \"flash_attn\": True,       # Use flash attention if available\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create the training dataset\n",
        "train_ds = PretrainDataset(\"petrain_data.jsonl\", tokenizer, max_length=model_config[\"max_seq_len\"])\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|endoftext|>\n",
            "<|endoftext|>\n",
            "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.bos_token)\n",
        "print(tokenizer.eos_token)\n",
        "print(tokenizer)\n",
        "\n",
        "# item = next(iter(train_ds))\n",
        "# print(item)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRMSNorm\u001b[39;00m(\u001b[43mtorch\u001b[49m.nn.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim: \u001b[38;5;28mint\u001b[39m, eps: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
            "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float):\n",
        "        super().__init__()\n",
        "        self.eps = eps  # Small constant for numerical stability\n",
        "        self.weight = nn.Parameter(torch.ones(dim)) # Learnable scaling parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the root mean square (RMS) and normalize\n",
        "        return self.weight * (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)).type_as(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Bienvenue dans Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ab559df54652bb95e07787d59a59d160dc53acec48e26b004f7b380e6f234b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
