{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyFou/testColab/blob/main/tinyllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ful9_femXKTv"
      },
      "source": [
        "Source du tuto\n",
        "https://levelup.gitconnected.com/building-a-perfect-million-parameter-llm-like-chatgpt-in-python-3b16e26b4139\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6mcGGTOGMS_"
      },
      "source": [
        "# Creation du fichier jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "p8RxLchItBQ9",
        "outputId": "f93419bb-8fb6-4ee0-a80c-4b17860ebf92"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8996f39d038>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./sft_data_en.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# faire un ctrl C en fonction de la taille souhaité du jdd\n",
        "#!wget https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl\n",
        "\n",
        "import os,urllib\n",
        "url = 'https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl'\n",
        "filename = './sft_data_en.jsonl'\n",
        "if not os.path.isfile(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9ig_9zGG3G"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "def remove_last_line(filepath):\n",
        "    \"\"\"Removes the last line of a large file efficiently.\n",
        "\n",
        "    Args:\n",
        "      filepath: The path to the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb+') as f:\n",
        "            f.seek(-2, os.SEEK_END)  # Go to the second-to-last byte\n",
        "            while f.read(1) != b'\\n':\n",
        "                f.seek(-2, os.SEEK_CUR)  # Back up two bytes\n",
        "                if f.tell() == 0:\n",
        "                    # Handle the case where there's only one line or no newline chars.\n",
        "                    f.truncate(0)\n",
        "                    return\n",
        "            f.truncate()\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing last line from {filepath}: {e}\")\n",
        "        return\n",
        "\n",
        "remove_last_line(\"sft_data_en.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xBmPTblYtEvG"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "!sed -i '$d' sft_data_en.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DuV4orGG3I",
        "outputId": "7cd0adc8-d844-4594-8df8-6692ef9cb67d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading JSONL file: 29218it [00:01, 16275.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import json\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# file_path = \"sft_data_en.jsonl\" # Downloaded filepath (23 GB RAM REQUIRED!!!!!!!!!!!!!!!!!!!!!)\n",
        "\n",
        "# # Read the JSONL file with tqdm progress bar\n",
        "# data = []\n",
        "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "#     # Iterate through each line of the JSONL file\n",
        "#     for line in tqdm(file, desc=\"Loading JSONL file\"):\n",
        "#         # Parse each line as a JSON object and append to data\n",
        "#         data.append(json.loads(line))\n",
        "\n",
        "# print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKIpX2vWuPdP",
        "outputId": "bf7cd543-e093-4a75-ac2f-e6d7fd8c2297"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter JSONL file: 6849it [00:00, 7046.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New JSONL file saved as petrain_data.jsonl\n",
            "275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##### retravaille le fichier de données : ligne de 512 caratère, supression (en grande partie) des lignes avec caratère asiatique\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "file_path = \"sft_data_en.jsonl\"\n",
        "\n",
        "# Define the output file name\n",
        "output_file = \"petrain_data.jsonl\"\n",
        "\n",
        "lenAfterFilter = 0\n",
        "# Write data to JSONL format\n",
        "asian_char_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3040-\\u30FF\\uAC00-\\uD7AF]')\n",
        "with open(file_path, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for line in tqdm(infile, desc=\"Filter JSONL file\"):\n",
        "        item = json.loads(line)\n",
        "        io_length = len(item['input']) + len(item['output'])\n",
        "        if io_length < 512 and not asian_char_pattern.search(line):\n",
        "            json.dump({\"text\": f\"{item['input']}\\n{item['output']}\"}, outfile, ensure_ascii=False)\n",
        "            outfile.write(\"\\n\")  # Newline for each JSONL entry\n",
        "            lenAfterFilter+=1\n",
        "print(f\"New JSONL file saved as {output_file}\")\n",
        "print(lenAfterFilter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF9YBGBGGUU0"
      },
      "source": [
        "# Creation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ub0zYwlbGG3K"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wD1V_40nGG3L"
      },
      "outputs": [],
      "source": [
        "class PretrainDataset(Dataset):\n",
        "    \"\"\"Dataset for pretraining.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Initializes the PretrainDataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the JSONL data file.\n",
        "            tokenizer: The tokenizer to use.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = self.load_data(data_path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        \"\"\"\n",
        "        Loads data from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the JSONL file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of samples loaded from the file.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):  # enumerate starts at 1 for line_num\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input (X), target (Y), and loss mask.\n",
        "        \"\"\"\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        # Construct the input text, including BOS and EOS tokens.\n",
        "        text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',  # Pad to max_length\n",
        "            truncation=True,  # Truncate to max_length\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "        input_ids = encoding.input_ids.squeeze()  # Remove extra dimension\n",
        "        loss_mask = (input_ids != self.tokenizer.pad_token_id)  # Create loss mask (ignore padding)\n",
        "\n",
        "        # Create input (X) and target (Y) tensors, shifting by one position.\n",
        "        X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
        "        Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
        "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n",
        "        return X, Y, loss_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.bos_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vI_EhrTeGG3L"
      },
      "outputs": [],
      "source": [
        "# Configuration for our Transformer model\n",
        "model_config = {\n",
        "    \"vocab_size\": 6400,       # Size of the vocabulary\n",
        "    \"dim\": 512,               # Dimensionality of the embeddings and hidden states\n",
        "    \"n_heads\": 8,             # Number of attention heads\n",
        "    \"n_kv_heads\": 2,          # Number of key-value heads (as specified in the LMConfig)\n",
        "    \"norm_eps\": 1e-5,         # Epsilon for RMSNorm\n",
        "    \"dropout\": 0.0,           # Dropout probability\n",
        "    \"max_seq_len\": 1024,      # Maximum sequence length\n",
        "    \"rope_theta\": 10000.0,    # Theta parameter for RoPE\n",
        "    \"multiple_of\": 64,        # Used for hidden dimension calculation in FFN\n",
        "    \"hidden_dim\": None,       # Hidden dimension of the FFN (calculated if None)\n",
        "    \"n_layers\": 8,            # Number of Transformer blocks\n",
        "    \"flash_attn\": True,       # Use flash attention if available\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zv4O5htUGG3M"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_bos_token = False\n",
        "tokenizer.add_eos_token = False\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Create the training dataset\n",
        "train_ds = PretrainDataset(\"petrain_data.jsonl\", tokenizer, max_length=model_config[\"max_seq_len\"])\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQIuPHRPGG3M",
        "outputId": "ac16612b-b7ba-4b71-e4a3-28b15ed0af9c"
      },
      "outputs": [],
      "source": [
        "x,y,mask = next(iter(train_ds))\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(mask.shape)\n",
        "print(\"x=>\",x)\n",
        "print(\"y=>\",y)\n",
        "print(\"x=>\",tokenizer.decode(x))\n",
        "print(\"y=>\",tokenizer.decode(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([50256, 20682,    73,   454, 50256, 50257, 50257, 50257, 50257, 50257])\n",
            "tensor([ True,  True,  True,  True,  True, False, False, False, False, False])\n",
            "tensor([50256, 20682,    73,   454, 50256, 50257, 50257, 50257, 50257])\n",
            "tensor([20682,    73,   454, 50256, 50257, 50257, 50257, 50257, 50257])\n",
            "tensor([1, 1, 1, 1, 0, 0, 0, 0, 0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fcyri\\AppData\\Local\\Temp\\ipykernel_1420\\2342458088.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
            "C:\\Users\\fcyri\\AppData\\Local\\Temp\\ipykernel_1420\\2342458088.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
            "C:\\Users\\fcyri\\AppData\\Local\\Temp\\ipykernel_1420\\2342458088.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n"
          ]
        }
      ],
      "source": [
        " # Construct the input text, including BOS and EOS tokens.\n",
        "text = f\"{tokenizer.bos_token}{'Bonjour'}{tokenizer.eos_token}\"\n",
        "encoding = tokenizer(\n",
        "    text,\n",
        "    max_length=10,\n",
        "    padding='max_length',  # Pad to max_length\n",
        "    truncation=True,  # Truncate to max_length\n",
        "    return_tensors='pt'  # Return PyTorch tensors\n",
        ")\n",
        "input_ids = encoding.input_ids.squeeze()  # Remove extra dimension\n",
        "loss_mask = (input_ids != tokenizer.pad_token_id)  # Create loss mask (ignore padding)\n",
        "print(input_ids)\n",
        "print(loss_mask)\n",
        "\n",
        "# Create input (X) and target (Y) tensors, shifting by one position.\n",
        "X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
        "Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
        "loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n",
        "\n",
        "print(X)\n",
        "print(Y)\n",
        "print(loss_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPNOVR4JGb4u"
      },
      "source": [
        "# Creation du model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds9876IQGG3N",
        "outputId": "bca61f76-197c-4ba9-a54b-93e3e60bcfae"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRMSNorm\u001b[39;00m(\u001b[43mtorch\u001b[49m.nn.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim: \u001b[38;5;28mint\u001b[39m, eps: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
            "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float):\n",
        "        super().__init__()\n",
        "        self.eps = eps  # Small constant for numerical stability\n",
        "        self.weight = nn.Parameter(torch.ones(dim)) # Learnable scaling parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the root mean square (RMS) and normalize\n",
        "        return self.weight * (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)).type_as(x)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Bienvenue dans Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ab559df54652bb95e07787d59a59d160dc53acec48e26b004f7b380e6f234b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
