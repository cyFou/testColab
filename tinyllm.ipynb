{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyFou/testColab/blob/main/tinyllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ful9_femXKTv"
      },
      "source": [
        "Source du tuto\n",
        "https://levelup.gitconnected.com/building-a-perfect-million-parameter-llm-like-chatgpt-in-python-3b16e26b4139\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6mcGGTOGMS_"
      },
      "source": [
        "# Creation du fichier jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "p8RxLchItBQ9",
        "outputId": "f93419bb-8fb6-4ee0-a80c-4b17860ebf92"
      },
      "outputs": [],
      "source": [
        "# faire un ctrl C en fonction de la taille souhaité du jdd\n",
        "#!wget https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl\n",
        "\n",
        "import os,urllib\n",
        "url = 'https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl'\n",
        "filename = './sft_data_en.jsonl'\n",
        "if not os.path.isfile(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9ig_9zGG3G"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "def remove_last_line(filepath):\n",
        "    \"\"\"Removes the last line of a large file efficiently.\n",
        "\n",
        "    Args:\n",
        "      filepath: The path to the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb+') as f:\n",
        "            f.seek(-2, os.SEEK_END)  # Go to the second-to-last byte\n",
        "            while f.read(1) != b'\\n':\n",
        "                f.seek(-2, os.SEEK_CUR)  # Back up two bytes\n",
        "                if f.tell() == 0:\n",
        "                    # Handle the case where there's only one line or no newline chars.\n",
        "                    f.truncate(0)\n",
        "                    return\n",
        "            f.truncate()\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing last line from {filepath}: {e}\")\n",
        "        return\n",
        "\n",
        "remove_last_line(\"sft_data_en.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBmPTblYtEvG"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "!sed -i '$d' sft_data_en.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DuV4orGG3I",
        "outputId": "7cd0adc8-d844-4594-8df8-6692ef9cb67d"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# file_path = \"sft_data_en.jsonl\" # Downloaded filepath (23 GB RAM REQUIRED!!!!!!!!!!!!!!!!!!!!!)\n",
        "\n",
        "# # Read the JSONL file with tqdm progress bar\n",
        "# data = []\n",
        "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "#     # Iterate through each line of the JSONL file\n",
        "#     for line in tqdm(file, desc=\"Loading JSONL file\"):\n",
        "#         # Parse each line as a JSON object and append to data\n",
        "#         data.append(json.loads(line))\n",
        "\n",
        "# print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKIpX2vWuPdP",
        "outputId": "bf7cd543-e093-4a75-ac2f-e6d7fd8c2297"
      },
      "outputs": [],
      "source": [
        "\n",
        "##### retravaille le fichier de données : ligne de 512 caratère, supression (en grande partie) des lignes avec caratère asiatique\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "file_path = \"sft_data_en.jsonl\"\n",
        "\n",
        "# Define the output file name\n",
        "output_file = \"petrain_data.jsonl\"\n",
        "\n",
        "lenAfterFilter = 0\n",
        "# Write data to JSONL format\n",
        "asian_char_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3040-\\u30FF\\uAC00-\\uD7AF]')\n",
        "with open(file_path, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for line in tqdm(infile, desc=\"Filter JSONL file\"):\n",
        "        item = json.loads(line)\n",
        "        io_length = len(item['input']) + len(item['output'])\n",
        "        if io_length < 512 and not asian_char_pattern.search(line):\n",
        "            json.dump({\"text\": f\"{item['input']}\\n{item['output']}\"}, outfile, ensure_ascii=False)\n",
        "            outfile.write(\"\\n\")  # Newline for each JSONL entry\n",
        "            lenAfterFilter+=1\n",
        "print(f\"New JSONL file saved as {output_file}\")\n",
        "print(lenAfterFilter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF9YBGBGGUU0"
      },
      "source": [
        "# Creation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ub0zYwlbGG3K"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\developpement\\01-devPython\\03-workspace\\LivreBuildingTransformerModels\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wD1V_40nGG3L"
      },
      "outputs": [],
      "source": [
        "class PretrainDataset(Dataset):\n",
        "    \"\"\"Dataset for pretraining.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Initializes the PretrainDataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the JSONL data file.\n",
        "            tokenizer: The tokenizer to use.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = self.load_data(data_path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        \"\"\"\n",
        "        Loads data from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the JSONL file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of samples loaded from the file.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):  # enumerate starts at 1 for line_num\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input (X), target (Y), and loss mask.\n",
        "        \"\"\"\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        # Construct the input text, including BOS and EOS tokens.\n",
        "        text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',  # Pad to max_length\n",
        "            truncation=True,  # Truncate to max_length\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "        input_ids = encoding.input_ids.squeeze()  # Remove extra dimension\n",
        "        loss_mask = (input_ids != self.tokenizer.pad_token_id)  # Create loss mask (ignore padding)\n",
        "\n",
        "        # Create input (X) and target (Y) tensors, shifting by one position.\n",
        "        X = input_ids[:-1].clone()\n",
        "        Y = input_ids[1:].clone()\n",
        "        loss_mask = loss_mask[1:].clone()  # Shift loss mask as well\n",
        "        return X, Y, loss_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vI_EhrTeGG3L"
      },
      "outputs": [],
      "source": [
        "# # Configuration for our Transformer model\n",
        "# model_config = {\n",
        "#     \"vocab_size\": 6400,       # Size of the vocabulary\n",
        "#     \"dim\": 512,               # Dimensionality of the embeddings and hidden states\n",
        "#     \"n_heads\": 8,             # Number of attention heads\n",
        "#     \"n_kv_heads\": 2,          # Number of key-value heads (as specified in the LMConfig)\n",
        "#     \"norm_eps\": 1e-5,         # Epsilon for RMSNorm\n",
        "#     \"dropout\": 0.0,           # Dropout probability\n",
        "#     \"max_seq_len\": 1024,      # Maximum sequence length\n",
        "#     \"rope_theta\": 10000.0,    # Theta parameter for RoPE\n",
        "#     \"multiple_of\": 64,        # Used for hidden dimension calculation in FFN\n",
        "#     \"hidden_dim\": None,       # Hidden dimension of the FFN (calculated if None)\n",
        "#     \"n_layers\": 8,            # Number of Transformer blocks\n",
        "#     \"flash_attn\": True,       # Use flash attention if available\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zv4O5htUGG3M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# tokenizer.add_bos_token = False\n",
        "# tokenizer.add_eos_token = False\n",
        "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# # Create the training dataset\n",
        "# train_ds = PretrainDataset(\"petrain_data.jsonl\", tokenizer, max_length=model_config[\"max_seq_len\"])\n",
        "\n",
        "# # Create the data loader\n",
        "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQIuPHRPGG3M",
        "outputId": "ac16612b-b7ba-4b71-e4a3-28b15ed0af9c"
      },
      "outputs": [],
      "source": [
        "# x,y,mask = next(iter(train_ds))\n",
        "# print(x.shape)\n",
        "# print(y.shape)\n",
        "# print(mask.shape)\n",
        "# print(\"x=>\",x)\n",
        "# print(\"y=>\",y)\n",
        "# print(\"x=>\",tokenizer.decode(x))\n",
        "# print(\"y=>\",tokenizer.decode(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPNOVR4JGb4u"
      },
      "source": [
        "# Creation du model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from transformers import PreTrainedModel\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from typing import Optional, Tuple, List\n",
        "from transformers import PretrainedConfig\n",
        "\n",
        "class LMConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    Configuration class for the language model.  Inherits from PretrainedConfig.\n",
        "    This configuration class stores all the hyperparameters of the model.\n",
        "\n",
        "    Attributes:\n",
        "        model_type (str): Model type identifier.\n",
        "        dim (int): Embedding dimension.\n",
        "        n_layers (int): Number of transformer layers.\n",
        "        n_heads (int): Number of attention heads.\n",
        "        n_kv_heads (int): Number of key-value attention heads (for multi-query attention).\n",
        "        vocab_size (int): Size of the vocabulary.\n",
        "        hidden_dim (int, optional): Hidden dimension of the feedforward network.\n",
        "            If None, it's calculated based on `dim` and `multiple_of`.\n",
        "        multiple_of (int): Used to calculate `hidden_dim` if `hidden_dim` is None.\n",
        "        norm_eps (float): Epsilon value for layer normalization.\n",
        "        max_seq_len (int): Maximum sequence length.\n",
        "        rope_theta (int): Theta value for rotary positional embeddings.\n",
        "        dropout (float): Dropout probability.\n",
        "        flash_attn (bool): Whether to use Flash Attention (if available).\n",
        "        use_moe (bool): Whether to use Mixture of Experts.\n",
        "        num_experts_per_tok (int): Number of experts per token (only if use_moe is True).\n",
        "        n_routed_experts (int): Total number of experts (only if use_moe is True).\n",
        "        n_shared_experts (bool): Whether to use shared experts (only if use_moe is True).\n",
        "        scoring_func (str): Scoring function for expert selection (only if use_moe is True).\n",
        "        aux_loss_alpha (float): Weight for the auxiliary loss (only if use_moe is True).\n",
        "        seq_aux (bool): Whether to compute aux loss at sequence level (only if use_moe is True).\n",
        "        norm_topk_prob (bool): Whether to normalize top-k probabilities (only if use_moe is True).\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = \"transformerlm\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 512,             # Embedding dimension\n",
        "        n_layers: int = 8,           # Number of transformer layers\n",
        "        n_heads: int = 8,           # Number of attention heads\n",
        "        n_kv_heads: int = 2,        # Number of key-value heads (multi-query attention)\n",
        "        vocab_size: int = 6400,     # Size of the vocabulary\n",
        "        hidden_dim: int = None,     # Hidden dimension of the FFN (calculated if None)\n",
        "        multiple_of: int = 64,      # Used for calculating hidden_dim\n",
        "        norm_eps: float = 1e-5,     # Epsilon value for layer normalization\n",
        "        max_seq_len: int = 8192,    # Maximum sequence length\n",
        "        rope_theta: int = 1e6,      # Theta for RoPE\n",
        "        dropout: float = 0.0,       # Dropout probability\n",
        "        flash_attn: bool = True,    # Use Flash Attention if available\n",
        "        num_experts_per_tok: int = 2, # Number of experts per token (unused)\n",
        "        n_routed_experts: int = 4,   # Total number of experts (unused)\n",
        "        n_shared_experts: bool = True,# Use shared experts (unused)\n",
        "        scoring_func: str = \"softmax\",# Expert scoring function (unused)\n",
        "        aux_loss_alpha: float = 0.1, # Weight for auxiliary loss (unused)\n",
        "        seq_aux: bool = True,        # Sequence-level auxiliary loss (unused)\n",
        "        norm_topk_prob: bool = True, # Normalize top-k probabilities (unused)\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.multiple_of = multiple_of\n",
        "        self.norm_eps = norm_eps\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.rope_theta = rope_theta\n",
        "        self.dropout = dropout\n",
        "        self.flash_attn = flash_attn\n",
        "        self.num_experts_per_tok = num_experts_per_tok\n",
        "        self.n_routed_experts = n_routed_experts\n",
        "        self.n_shared_experts = n_shared_experts\n",
        "        self.scoring_func = scoring_func\n",
        "        self.aux_loss_alpha = aux_loss_alpha\n",
        "        self.seq_aux = seq_aux\n",
        "        self.norm_topk_prob = norm_topk_prob\n",
        "\n",
        "        super().__init__(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ds9876IQGG3N",
        "outputId": "bca61f76-197c-4ba9-a54b-93e3e60bcfae"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float):\n",
        "        super().__init__()\n",
        "        self.eps = eps  # Small constant for numerical stability\n",
        "        self.weight = nn.Parameter(torch.ones(dim)) # Learnable scaling parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate the root mean square (RMS) and normalize\n",
        "        return self.weight * (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)).type_as(x)\n",
        "        \n",
        "def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
        "    \"\"\"\n",
        "    Precomputes complex exponentials (cis) for rotary positional embeddings.\n",
        "\n",
        "    Args:\n",
        "        dim: Dimensionality of the embeddings.\n",
        "        end: Maximum sequence length.\n",
        "        theta: Scaling factor for frequencies.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Precomputed complex exponentials.\n",
        "    \"\"\"\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore # Sequence indices\n",
        "    freqs = torch.outer(t, freqs).float()  # type: ignore # Outer product to get frequencies for each position\n",
        "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64 # Create complex exponentials\n",
        "    return pos_cis\n",
        "\n",
        "\n",
        "def apply_rotary_emb(xq, xk, pos_cis):\n",
        "    \"\"\"\n",
        "    Applies rotary positional embeddings to query (xq) and key (xk) tensors.\n",
        "\n",
        "    Args:\n",
        "        xq: Query tensor.\n",
        "        xk: Key tensor.\n",
        "        pos_cis: Precomputed complex exponentials.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: Query and key tensors with rotary embeddings applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def unite_shape(pos_cis, x):\n",
        "        # Reshape pos_cis to have compatible dimensions with x for broadcasting.\n",
        "        ndim = x.ndim\n",
        "        assert 0 <= 1 < ndim\n",
        "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
        "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "        return pos_cis.view(*shape)\n",
        "\n",
        "    # Reshape and convert to complex numbers for efficient multiplication.\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    pos_cis = unite_shape(pos_cis, xq_)  # Reshape pos_cis for broadcasting\n",
        "    # Apply rotary embeddings via complex number multiplication.\n",
        "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)  # Ensure output type matches input\n",
        "\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repeats the key-value pairs for multi-query attention.\n",
        "\n",
        "    Args:\n",
        "        x: Key-value tensor.\n",
        "        n_rep: Number of times to repeat each head.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Key-value tensor with repeated heads.\n",
        "    \"\"\"\n",
        "    bs, slen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    # Expand and reshape to repeat the key-value heads.\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements multi-head attention with rotary positional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args: LMConfig):\n",
        "        super().__init__()\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        assert args.n_heads % self.n_kv_heads == 0\n",
        "        self.n_local_heads = args.n_heads  # Total number of attention heads\n",
        "        self.n_local_kv_heads = self.n_kv_heads  # Number of key-value attention heads\n",
        "        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # Repetition factor for key-value heads\n",
        "        self.head_dim = args.dim // args.n_heads  # Dimension of each attention head\n",
        "        # Linear projections for query, key, and value.\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)  # Output projection\n",
        "        self.attn_dropout = nn.Dropout(args.dropout)  # Dropout for attention weights\n",
        "        self.resid_dropout = nn.Dropout(args.dropout)  # Dropout for the residual connection\n",
        "        self.dropout = args.dropout\n",
        "        # Check for Flash Attention availability (requires PyTorch >= 2.0).\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
        "\n",
        "        # Causal mask for autoregressive decoding.\n",
        "        mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
        "        mask = torch.triu(mask, diagonal=1)  # Upper triangular mask\n",
        "        self.register_buffer(\"mask\", mask, persistent=False)  # Register as a buffer (not a parameter)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                pos_cis: torch.Tensor,\n",
        "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
        "                use_cache=False):\n",
        "        bsz, seq_len, _ = x.shape\n",
        "        # Apply linear projections to get query, key, and value.\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "        # Reshape for multi-head attention.\n",
        "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
        "\n",
        "        # Apply rotary positional embeddings.\n",
        "        xq, xk = apply_rotary_emb(xq, xk, pos_cis)\n",
        "\n",
        "        # KV Cache implementation\n",
        "        if past_key_value is not None:\n",
        "            xk = torch.cat([past_key_value[0], xk], dim=1)  # Concatenate with cached keys\n",
        "            xv = torch.cat([past_key_value[1], xv], dim=1)  # Concatenate with cached values\n",
        "        past_kv = (xk, xv) if use_cache else None  # Store current keys and values for caching\n",
        "\n",
        "        # Repeat key-value pairs for multi-query attention.\n",
        "        xq, xk, xv = (\n",
        "            xq.transpose(1, 2),\n",
        "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
        "            repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
        "        )\n",
        "\n",
        "        # Attention mechanism selection: Flash Attention (if available) or standard attention.\n",
        "        if self.flash and seq_len != 1:\n",
        "            dropout_p = self.dropout if self.training else 0.0  # Dropout only during training\n",
        "            output = F.scaled_dot_product_attention(\n",
        "                xq, xk, xv,\n",
        "                attn_mask=None,  # No explicit mask needed (causal masking is handled internally)\n",
        "                dropout_p=dropout_p,\n",
        "                is_causal=True  # Enforce causal attention\n",
        "            )\n",
        "        else:\n",
        "            # Standard attention\n",
        "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Calculate attention scores\n",
        "            scores += self.mask[:, :, :seq_len, :seq_len]  # Apply causal mask\n",
        "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)  # Softmax to get attention weights\n",
        "            scores = self.attn_dropout(scores)  # Apply dropout\n",
        "            output = scores @ xv  # Weighted sum of values\n",
        "\n",
        "        # Reshape and apply output projection.\n",
        "        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)\n",
        "        output = self.resid_dropout(self.wo(output))\n",
        "        return output, past_kv\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the feedforward network (FFN) used in each transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LMConfig):\n",
        "        super().__init__()\n",
        "        # Compute hidden dimension for FFN.\n",
        "        if config.hidden_dim is None:\n",
        "            hidden_dim = 4 * config.dim\n",
        "            hidden_dim = int(2 * hidden_dim / 3)\n",
        "            config.hidden_dim = config.multiple_of * ((hidden_dim + config.multiple_of - 1) // config.multiple_of)\n",
        "        # Linear layers with SiLU activation.\n",
        "        self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
        "        self.w3 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)  # Dropout after the FFN\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFN transformation:  x -> SiLU(xW1) * (xW3) -> (result)W2 -> dropout\n",
        "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_id: int, config: LMConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.dim = config.dim\n",
        "        self.head_dim = config.dim // config.n_heads\n",
        "        self.attention = Attention(config)  # Multi-head attention\n",
        "\n",
        "        self.layer_id = layer_id\n",
        "        # Layer normalization for attention and FFN.\n",
        "        self.attention_norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
        "        # Feedforward network.\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, pos_cis, past_key_value=None, use_cache=False):\n",
        "        # Attention block with residual connection.\n",
        "        h_attn, past_kv = self.attention(\n",
        "            self.attention_norm(x),  # Normalize input before attention\n",
        "            pos_cis,  # Rotary positional embeddings\n",
        "            past_key_value=past_key_value,  # Pass cached key-value pairs\n",
        "            use_cache=use_cache  # Whether to use caching\n",
        "        )\n",
        "        h = x + h_attn  # Residual connection\n",
        "        # Feedforward block with residual connection.\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))  # Normalize input before FFN\n",
        "        return out, past_kv\n",
        "\n",
        "\n",
        "class TransformerLM(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    The main Transformer language model.\n",
        "    \"\"\"\n",
        "    config_class = LMConfig  # Use LMConfig as the configuration class\n",
        "\n",
        "    def __init__(self, params: LMConfig = None):\n",
        "        self.params = params or LMConfig()  # Use default config if none provided\n",
        "        super().__init__(self.params)  # Initialize PreTrainedModel\n",
        "        self.vocab_size, self.n_layers = params.vocab_size, params.n_layers\n",
        "        # Token embeddings.\n",
        "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "        self.dropout = nn.Dropout(params.dropout)  # Dropout after embeddings\n",
        "        # Transformer blocks.\n",
        "        self.layers = nn.ModuleList([TransformerBlock(l, params) for l in range(self.n_layers)])\n",
        "        # Final layer normalization.\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        # Output layer (maps from hidden states to logits).\n",
        "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
        "        # Tie token embeddings and output weights.\n",
        "        #self.tok_embeddings.weight = self.output.weight\n",
        "        # Precompute and register rotary positional embeddings.\n",
        "        self.register_buffer(\"pos_cis\",\n",
        "                             precompute_pos_cis(dim=params.dim // params.n_heads, theta=params.rope_theta),\n",
        "                             persistent=False)\n",
        "        self.OUT = CausalLMOutputWithPast()  # Use CausalLMOutputWithPast for output\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids: Optional[torch.Tensor] = None,\n",
        "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
        "                use_cache: bool = False,\n",
        "                **args):\n",
        "\n",
        "        past_key_values = past_key_values or [None] * len(self.layers)  # Initialize empty cache if None\n",
        "        start_pos = args.get('start_pos', 0)  # Get starting position for sequence generation\n",
        "\n",
        "        h = self.dropout(self.tok_embeddings(input_ids))  # Get token embeddings and apply dropout\n",
        "        pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]  # Get relevant rotary embeddings\n",
        "\n",
        "        past_kvs = []  # Store past key-value pairs for caching\n",
        "        for l, layer in enumerate(self.layers):\n",
        "            h, past_kv = layer(\n",
        "                h, pos_cis,\n",
        "                past_key_value=past_key_values[l],  # Pass cached key-value pairs\n",
        "                use_cache=use_cache\n",
        "            )\n",
        "            past_kvs.append(past_kv)  # Store updated key-value pairs\n",
        "\n",
        "        logits = self.output(self.norm(h))  # Final layer normalization and output projection\n",
        "\n",
        "        # Set output attributes using __setitem__\n",
        "        self.OUT.__setitem__('logits', logits)\n",
        "        self.OUT.__setitem__('past_key_values', past_kvs)\n",
        "        return self.OUT  # Return CausalLMOutputWithPast object\n",
        "\n",
        "    #@torch.inference_mode()\n",
        "    def generate(self, input_ids, eos_token_id=2, max_new_tokens=1024, temperature=0.75, top_p=0.90,\n",
        "                 stream=False, rp=1., use_cache=True, pad_token_id=0, **args):\n",
        "        \"\"\"\n",
        "        Generates text from the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Initial input token IDs.\n",
        "            eos_token_id: End-of-sequence token ID.\n",
        "            max_new_tokens: Maximum number of tokens to generate.\n",
        "            temperature: Sampling temperature.\n",
        "            top_p: Top-p (nucleus) sampling probability.\n",
        "            stream: Whether to stream the output (yield tokens one by one).\n",
        "            rp: Repetition penalty.\n",
        "            use_cache: Whether to use key-value caching.\n",
        "            pad_token_id: pad token id.\n",
        "            **args: Additional arguments passed to the forward method.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Generated sequence of token IDs.\n",
        "                         (if stream=False)\n",
        "            Generator[torch.Tensor, None, None]:  Generated tokens.\n",
        "                                        (if stream=True)\n",
        "        \"\"\"\n",
        "        # Stream generation\n",
        "        if stream:\n",
        "            return self._stream(input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args)\n",
        "\n",
        "        # Direct generation (collects all tokens at once)\n",
        "        generated = []\n",
        "        for i in range(input_ids.size(0)):\n",
        "            # remove padding\n",
        "            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)\n",
        "            # generate tokens\n",
        "            out = self._stream(non_pad, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args)\n",
        "            # collect the generated token one-by-one\n",
        "            tokens_list = [tokens[:, -1:] for tokens in out]\n",
        "            gen = torch.cat(tokens_list, dim=-1) if tokens_list else non_pad\n",
        "            # concat the input and generated tokens together\n",
        "            full_sequence = torch.cat([non_pad, gen], dim=-1)\n",
        "            generated.append(full_sequence)\n",
        "        # find the longest sequence\n",
        "        max_length = max(seq.size(1) for seq in generated)\n",
        "        # padding the sequences\n",
        "        generated = [\n",
        "            torch.cat(\n",
        "                [seq, torch.full((1, max_length - seq.size(1)), pad_token_id, dtype=seq.dtype, device=seq.device)],\n",
        "                dim=-1)\n",
        "            for seq in generated\n",
        "        ]\n",
        "        # concatenate all generated tensors together\n",
        "        return torch.cat(generated, dim=0)\n",
        "\n",
        "    def _stream(self, input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args):\n",
        "        \"\"\"\n",
        "        Helper function for streaming text generation.\n",
        "        \"\"\"\n",
        "        start, first_seq, past_kvs = input_ids.shape[1], True, None\n",
        "        while input_ids.shape[1] < max_new_tokens - 1:\n",
        "            if first_seq or not use_cache:\n",
        "                # For the first sequence or when not using cache, process the entire input sequence.\n",
        "                out, first_seq = self(input_ids, past_key_values=past_kvs, use_cache=use_cache, **args), False\n",
        "            else:\n",
        "                # For subsequent sequences with caching, process only the last token.\n",
        "                out = self(input_ids[:, -1:], past_key_values=past_kvs, use_cache=use_cache,\n",
        "                           start_pos=input_ids.shape[1] - 1, **args)\n",
        "            logits, past_kvs = out.logits[:, -1, :], out.past_key_values  # Get logits and updated cache\n",
        "\n",
        "            # Apply repetition penalty.\n",
        "            logits[:, list(set(input_ids.tolist()[0]))] /= rp\n",
        "\n",
        "            # Apply temperature scaling.\n",
        "            logits /= (temperature + 1e-9)\n",
        "\n",
        "            # Apply top-p (nucleus) sampling.\n",
        "            if top_p is not None and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "                sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = False\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = -float('Inf')  # Set probabilities to -inf for filtered tokens\n",
        "\n",
        "            # Sample the next token.\n",
        "            input_ids_next = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)  # Append the new token to the sequence\n",
        "            yield input_ids[:, start:]  # Yield the generated tokens (excluding the initial input)\n",
        "\n",
        "            # Break if EOS token is generated.\n",
        "            if input_ids_next.item() == eos_token_id:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPNOVR4JGb4u"
      },
      "source": [
        "# boucle d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lr(current_step, total_steps, lr):\n",
        "    \"\"\"Calculates the learning rate using a cosine schedule.\"\"\"\n",
        "    return lr / 10 + 0.5 * lr * (1 + math.cos(math.pi * current_step / total_steps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "# device = \"cpu\"\n",
        "device = \"cuda\"\n",
        "lm_config = LMConfig() #max_seq_len=15)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "lm_config.vocab_size = tokenizer.vocab_size+1\n",
        "\n",
        "model = TransformerLM(lm_config)\n",
        "model = model.to(device) # Move the model to the GPU if available\n",
        "\n",
        "# Create the training dataset\n",
        "train_ds = PretrainDataset(\"petrain_data.jsonl\", tokenizer, max_length=lm_config.max_seq_len)\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, drop_last=False)\n",
        "\n",
        "# Define the optimizer (AdamW is a good choice)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Define the loss function (cross-entropy)\n",
        "loss_fct = nn.CrossEntropyLoss(reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1278 [00:17<6:16:50, 17.71s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m loss = loss/accumulation_steps \u001b[38;5;66;03m# divided by accumulation steps\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (step + \u001b[32m1\u001b[39m) % accumulation_steps == \u001b[32m0\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Gradient Clipping\u001b[39;00m\n\u001b[32m     37\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\developpement\\01-devPython\\03-workspace\\LivreBuildingTransformerModels\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\developpement\\01-devPython\\03-workspace\\LivreBuildingTransformerModels\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\developpement\\01-devPython\\03-workspace\\LivreBuildingTransformerModels\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "epochs = 1 # You can adjust the number of epochs\n",
        "iter_per_epoch = len(train_loader) #how many batches are there\n",
        "accumulation_steps = 8 #for gradient accumulation\n",
        "grad_clip = 1.0 #for gradient clipping\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    #for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
        "    step = 0\n",
        "    for (X, Y, loss_mask) in tqdm(train_loader):    \n",
        "        X = X.to(\"cuda\")\n",
        "        Y = Y.to(\"cuda\")\n",
        "        loss_mask = loss_mask.to(\"cuda\")\n",
        "\n",
        "        # Calculate the learning rate for the current step\n",
        "        lr = get_lr(epoch * iter_per_epoch + step, epochs * iter_per_epoch, 5e-4)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        # Forward pass\n",
        "        res = model(X)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fct(\n",
        "            res.logits.view(-1, res.logits.size(-1)),\n",
        "            Y.view(-1)\n",
        "        ).view(Y.size())\n",
        "        loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
        "        loss = loss/accumulation_steps # divided by accumulation steps\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            # Update the model's parameters\n",
        "            optimizer.step()\n",
        "            # Reset the gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        if step % 100 == 0:\n",
        "            spend_time = time.time() - start_time\n",
        "            print(\n",
        "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.12f} epoch_Time:{}min:'.format(\n",
        "                    epoch + 1,\n",
        "                    epochs,\n",
        "                    step,\n",
        "                    iter_per_epoch,\n",
        "                    loss.item() * accumulation_steps,\n",
        "                    optimizer.param_groups[-1]['lr'],\n",
        "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time //60))\n",
        "         \n",
        "        if(step + 1) % 100 == 0:\n",
        "            model.eval() #set to evaluation mode\n",
        "            torch.save(model.state_dict(), \"pretrain_model.pth\") #save state dict\n",
        "            model.train()# set back to training mode. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 14]) torch.Size([1, 14])\n",
            "tensor([[50256,    40,  1053,  2982,  1049,  1243,   546,   366,  2202, 22183,\n",
            "             1,   416,  7970,  2677]])\n",
            "tensor([[   40,  1053,  2982,  1049,  1243,   546,   366,  2202, 22183,     1,\n",
            "           416,  7970,  2677,   290]])\n",
            "tensor([   40,  1053,  2982,  1049,  1243,   546,   366,  2202, 22183,     1,\n",
            "          416,  7970,  2677,   290])\n",
            "torch.Size([1, 14, 50258])\n",
            "torch.Size([14, 50258])\n"
          ]
        }
      ],
      "source": [
        "# X,Y,loss_mask = next(iter(train_loader))\n",
        "# print(X.shape, Y.shape)\n",
        "# X = X.to(device)\n",
        "# Y = Y.to(device)\n",
        "# loss_mask = loss_mask.to(device)\n",
        "\n",
        "# # Forward pass\n",
        "# res = model(X)\n",
        "\n",
        "# # Calculate the loss\n",
        "# loss = loss_fct(\n",
        "#     res.logits.view(-1, res.logits.size(-1)),\n",
        "#     Y.view(-1)\n",
        "# ).view(Y.size())\n",
        "\n",
        "# print(X)\n",
        "# print(Y)\n",
        "# print(Y.view(-1))\n",
        "# print(res.logits.shape)\n",
        "# print(res.logits.view(-1, res.logits.size(-1)).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[15496,   995]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world priestsioned wonderedresp clashedazel Mikhail\n"
          ]
        }
      ],
      "source": [
        "# prompt = \"Hello world\"\n",
        "# max_new_tokens=10\n",
        "# temperature=0.85\n",
        "# top_p=0.9\n",
        "# input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "# print(input_ids)\n",
        "# output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, \n",
        "#                                 eos_token_id=tokenizer.eos_token_id)\n",
        "# print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Bienvenue dans Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ab559df54652bb95e07787d59a59d160dc53acec48e26b004f7b380e6f234b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
