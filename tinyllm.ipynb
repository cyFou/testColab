{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyFou/testColab/blob/main/tinyllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ful9_femXKTv"
      },
      "source": [
        "Source du tuto\n",
        "https://levelup.gitconnected.com/building-a-perfect-million-parameter-llm-like-chatgpt-in-python-3b16e26b4139\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation du fichier jeu de données"
      ],
      "metadata": {
        "id": "a6mcGGTOGMS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "p8RxLchItBQ9",
        "outputId": "f93419bb-8fb6-4ee0-a80c-4b17860ebf92"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8996f39d038>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./sft_data_en.jsonl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mread\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mblocknum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# faire un ctrl C en fonction de la taille souhaité du jdd\n",
        "#!wget https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl\n",
        "\n",
        "import os,urllib\n",
        "url = 'https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/resolve/master/sft_data_en.jsonl'\n",
        "filename = './sft_data_en.jsonl'\n",
        "if not os.path.isfile(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9ig_9zGG3G"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "def remove_last_line(filepath):\n",
        "    \"\"\"Removes the last line of a large file efficiently.\n",
        "\n",
        "    Args:\n",
        "      filepath: The path to the file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb+') as f:\n",
        "            f.seek(-2, os.SEEK_END)  # Go to the second-to-last byte\n",
        "            while f.read(1) != b'\\n':\n",
        "                f.seek(-2, os.SEEK_CUR)  # Back up two bytes\n",
        "                if f.tell() == 0:\n",
        "                    # Handle the case where there's only one line or no newline chars.\n",
        "                    f.truncate(0)\n",
        "                    return\n",
        "            f.truncate()\n",
        "    except OSError as e:\n",
        "        print(f\"Error removing last line from {filepath}: {e}\")\n",
        "        return\n",
        "\n",
        "remove_last_line(\"sft_data_en.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xBmPTblYtEvG"
      },
      "outputs": [],
      "source": [
        "# Supprimer la dernière ligne du fichier sft_data_en.jsonl car mal formaté du fait du Ctrl C\n",
        "!sed -i '$d' sft_data_en.jsonl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DuV4orGG3I",
        "outputId": "7cd0adc8-d844-4594-8df8-6692ef9cb67d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading JSONL file: 29218it [00:01, 16275.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# import json\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# file_path = \"sft_data_en.jsonl\" # Downloaded filepath (23 GB RAM REQUIRED!!!!!!!!!!!!!!!!!!!!!)\n",
        "\n",
        "# # Read the JSONL file with tqdm progress bar\n",
        "# data = []\n",
        "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "#     # Iterate through each line of the JSONL file\n",
        "#     for line in tqdm(file, desc=\"Loading JSONL file\"):\n",
        "#         # Parse each line as a JSON object and append to data\n",
        "#         data.append(json.loads(line))\n",
        "\n",
        "# print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKIpX2vWuPdP",
        "outputId": "bf7cd543-e093-4a75-ac2f-e6d7fd8c2297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Filter JSONL file: 6849it [00:00, 7046.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New JSONL file saved as petrain_data.jsonl\n",
            "275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##### retravaille le fichier de données : ligne de 512 caratère, supression (en grande partie) des lignes avec caratère asiatique\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "file_path = \"sft_data_en.jsonl\"\n",
        "\n",
        "# Define the output file name\n",
        "output_file = \"petrain_data.jsonl\"\n",
        "\n",
        "lenAfterFilter = 0\n",
        "# Write data to JSONL format\n",
        "asian_char_pattern = re.compile(r'[\\u4E00-\\u9FFF\\u3040-\\u30FF\\uAC00-\\uD7AF]')\n",
        "with open(file_path, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    for line in tqdm(infile, desc=\"Filter JSONL file\"):\n",
        "        item = json.loads(line)\n",
        "        io_length = len(item['input']) + len(item['output'])\n",
        "        if io_length < 512 and not asian_char_pattern.search(line):\n",
        "            json.dump({\"text\": f\"{item['input']}\\n{item['output']}\"}, outfile, ensure_ascii=False)\n",
        "            outfile.write(\"\\n\")  # Newline for each JSONL entry\n",
        "            lenAfterFilter+=1\n",
        "print(f\"New JSONL file saved as {output_file}\")\n",
        "print(lenAfterFilter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation du dataset"
      ],
      "metadata": {
        "id": "cF9YBGBGGUU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ub0zYwlbGG3K"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wD1V_40nGG3L"
      },
      "outputs": [],
      "source": [
        "class PretrainDataset(Dataset):\n",
        "    \"\"\"Dataset for pretraining.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Initializes the PretrainDataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the JSONL data file.\n",
        "            tokenizer: The tokenizer to use.\n",
        "            max_length (int): Maximum sequence length.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = self.load_data(data_path)\n",
        "\n",
        "    def load_data(self, path):\n",
        "        \"\"\"\n",
        "        Loads data from a JSONL file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the JSONL file.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of samples loaded from the file.\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f, 1):  # enumerate starts at 1 for line_num\n",
        "                data = json.loads(line.strip())\n",
        "                samples.append(data)\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a sample from the dataset.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input (X), target (Y), and loss mask.\n",
        "        \"\"\"\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        # Construct the input text, including BOS and EOS tokens.\n",
        "        text = f\"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',  # Pad to max_length\n",
        "            truncation=True,  # Truncate to max_length\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "        input_ids = encoding.input_ids.squeeze()  # Remove extra dimension\n",
        "        loss_mask = (input_ids != self.tokenizer.pad_token_id)  # Create loss mask (ignore padding)\n",
        "\n",
        "        # Create input (X) and target (Y) tensors, shifting by one position.\n",
        "        X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
        "        Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
        "        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n",
        "        return X, Y, loss_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vI_EhrTeGG3L"
      },
      "outputs": [],
      "source": [
        "# Configuration for our Transformer model\n",
        "model_config = {\n",
        "    \"vocab_size\": 6400,       # Size of the vocabulary\n",
        "    \"dim\": 512,               # Dimensionality of the embeddings and hidden states\n",
        "    \"n_heads\": 8,             # Number of attention heads\n",
        "    \"n_kv_heads\": 2,          # Number of key-value heads (as specified in the LMConfig)\n",
        "    \"norm_eps\": 1e-5,         # Epsilon for RMSNorm\n",
        "    \"dropout\": 0.0,           # Dropout probability\n",
        "    \"max_seq_len\": 1024,      # Maximum sequence length\n",
        "    \"rope_theta\": 10000.0,    # Theta parameter for RoPE\n",
        "    \"multiple_of\": 64,        # Used for hidden dimension calculation in FFN\n",
        "    \"hidden_dim\": None,       # Hidden dimension of the FFN (calculated if None)\n",
        "    \"n_layers\": 8,            # Number of Transformer blocks\n",
        "    \"flash_attn\": True,       # Use flash attention if available\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zv4O5htUGG3M"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_bos_token = False\n",
        "tokenizer.add_eos_token = False\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Create the training dataset\n",
        "train_ds = PretrainDataset(\"petrain_data.jsonl\", tokenizer, max_length=model_config[\"max_seq_len\"])\n",
        "\n",
        "# Create the data loader\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQIuPHRPGG3M",
        "outputId": "ac16612b-b7ba-4b71-e4a3-28b15ed0af9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1023])\n",
            "torch.Size([1023])\n",
            "torch.Size([1023])\n",
            "x=> tensor([50256, 11627,   974,  ..., 50257, 50257, 50257])\n",
            "y=> tensor([11627,   974,   262,  ..., 50257, 50257, 50257])\n",
            "x=> <|endoftext|>Extract the main points from the essay.\n",
            "The essay explores the impact of technology on human communication. It states that while technology has made communication more convenient, it has also led to a decrease in the quality of human interaction. The abundance of online communication has led to a decrease in social skills and the ability to read non-verbal cues. The essay concludes that it is important to not forget the value of face-to-face communication.<|endoftext|>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "y=> Extract the main points from the essay.\n",
            "The essay explores the impact of technology on human communication. It states that while technology has made communication more convenient, it has also led to a decrease in the quality of human interaction. The abundance of online communication has led to a decrease in social skills and the ability to read non-verbal cues. The essay concludes that it is important to not forget the value of face-to-face communication.<|endoftext|>[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c3a42496a638>:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(input_ids[:-1], dtype=torch.long)\n",
            "<ipython-input-5-c3a42496a638>:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y = torch.tensor(input_ids[1:], dtype=torch.long)\n",
            "<ipython-input-5-c3a42496a638>:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # Shift loss mask as well\n"
          ]
        }
      ],
      "source": [
        "x,y,mask = next(iter(train_ds))\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(mask.shape)\n",
        "print(\"x=>\",x)\n",
        "print(\"y=>\",y)\n",
        "print(\"x=>\",tokenizer.decode(x))\n",
        "print(\"y=>\",tokenizer.decode(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creation du model"
      ],
      "metadata": {
        "id": "RPNOVR4JGb4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds9876IQGG3N",
        "outputId": "bca61f76-197c-4ba9-a54b-93e3e60bcfae"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRMSNorm\u001b[39;00m(\u001b[43mtorch\u001b[49m.nn.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim: \u001b[38;5;28mint\u001b[39m, eps: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
            "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
        "    \"\"\"\n",
        "    Precomputes complex exponentials (cis) for rotary positional embeddings.\n",
        "\n",
        "    Args:\n",
        "        dim: Dimensionality of the embeddings.\n",
        "        end: Maximum sequence length.\n",
        "        theta: Scaling factor for frequencies.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Precomputed complex exponentials.\n",
        "    \"\"\"\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore # Sequence indices\n",
        "    freqs = torch.outer(t, freqs).float()  # type: ignore # Outer product to get frequencies for each position\n",
        "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64 # Create complex exponentials\n",
        "    return pos_cis\n",
        "\n",
        "\n",
        "def apply_rotary_emb(xq, xk, pos_cis):\n",
        "    \"\"\"\n",
        "    Applies rotary positional embeddings to query (xq) and key (xk) tensors.\n",
        "\n",
        "    Args:\n",
        "        xq: Query tensor.\n",
        "        xk: Key tensor.\n",
        "        pos_cis: Precomputed complex exponentials.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: Query and key tensors with rotary embeddings applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def unite_shape(pos_cis, x):\n",
        "        # Reshape pos_cis to have compatible dimensions with x for broadcasting.\n",
        "        ndim = x.ndim\n",
        "        assert 0 <= 1 < ndim\n",
        "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
        "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "        return pos_cis.view(*shape)\n",
        "\n",
        "    # Reshape and convert to complex numbers for efficient multiplication.\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    pos_cis = unite_shape(pos_cis, xq_)  # Reshape pos_cis for broadcasting\n",
        "    # Apply rotary embeddings via complex number multiplication.\n",
        "    xq_out = torch.view_as_real(xq_ * pos_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * pos_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)  # Ensure output type matches input\n",
        "\n",
        "\n",
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repeats the key-value pairs for multi-query attention.\n",
        "\n",
        "    Args:\n",
        "        x: Key-value tensor.\n",
        "        n_rep: Number of times to repeat each head.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Key-value tensor with repeated heads.\n",
        "    \"\"\"\n",
        "    bs, slen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    # Expand and reshape to repeat the key-value heads.\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements multi-head attention with rotary positional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args: LMConfig):\n",
        "        super().__init__()\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        assert args.n_heads % self.n_kv_heads == 0\n",
        "        self.n_local_heads = args.n_heads  # Total number of attention heads\n",
        "        self.n_local_kv_heads = self.n_kv_heads  # Number of key-value attention heads\n",
        "        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # Repetition factor for key-value heads\n",
        "        self.head_dim = args.dim // args.n_heads  # Dimension of each attention head\n",
        "        # Linear projections for query, key, and value.\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)  # Output projection\n",
        "        self.attn_dropout = nn.Dropout(args.dropout)  # Dropout for attention weights\n",
        "        self.resid_dropout = nn.Dropout(args.dropout)  # Dropout for the residual connection\n",
        "        self.dropout = args.dropout\n",
        "        # Check for Flash Attention availability (requires PyTorch >= 2.0).\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn\n",
        "\n",
        "        # Causal mask for autoregressive decoding.\n",
        "        mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
        "        mask = torch.triu(mask, diagonal=1)  # Upper triangular mask\n",
        "        self.register_buffer(\"mask\", mask, persistent=False)  # Register as a buffer (not a parameter)\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                pos_cis: torch.Tensor,\n",
        "                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
        "                use_cache=False):\n",
        "        bsz, seq_len, _ = x.shape\n",
        "        # Apply linear projections to get query, key, and value.\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "        # Reshape for multi-head attention.\n",
        "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
        "\n",
        "        # Apply rotary positional embeddings.\n",
        "        xq, xk = apply_rotary_emb(xq, xk, pos_cis)\n",
        "\n",
        "        # KV Cache implementation\n",
        "        if past_key_value is not None:\n",
        "            xk = torch.cat([past_key_value[0], xk], dim=1)  # Concatenate with cached keys\n",
        "            xv = torch.cat([past_key_value[1], xv], dim=1)  # Concatenate with cached values\n",
        "        past_kv = (xk, xv) if use_cache else None  # Store current keys and values for caching\n",
        "\n",
        "        # Repeat key-value pairs for multi-query attention.\n",
        "        xq, xk, xv = (\n",
        "            xq.transpose(1, 2),\n",
        "            repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
        "            repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
        "        )\n",
        "\n",
        "        # Attention mechanism selection: Flash Attention (if available) or standard attention.\n",
        "        if self.flash and seq_len != 1:\n",
        "            dropout_p = self.dropout if self.training else 0.0  # Dropout only during training\n",
        "            output = F.scaled_dot_product_attention(\n",
        "                xq, xk, xv,\n",
        "                attn_mask=None,  # No explicit mask needed (causal masking is handled internally)\n",
        "                dropout_p=dropout_p,\n",
        "                is_causal=True  # Enforce causal attention\n",
        "            )\n",
        "        else:\n",
        "            # Standard attention\n",
        "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Calculate attention scores\n",
        "            scores += self.mask[:, :, :seq_len, :seq_len]  # Apply causal mask\n",
        "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)  # Softmax to get attention weights\n",
        "            scores = self.attn_dropout(scores)  # Apply dropout\n",
        "            output = scores @ xv  # Weighted sum of values\n",
        "\n",
        "        # Reshape and apply output projection.\n",
        "        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)\n",
        "        output = self.resid_dropout(self.wo(output))\n",
        "        return output, past_kv\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the feedforward network (FFN) used in each transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LMConfig):\n",
        "        super().__init__()\n",
        "        # Compute hidden dimension for FFN.\n",
        "        if config.hidden_dim is None:\n",
        "            hidden_dim = 4 * config.dim\n",
        "            hidden_dim = int(2 * hidden_dim / 3)\n",
        "            config.hidden_dim = config.multiple_of * ((hidden_dim + config.multiple_of - 1) // config.multiple_of)\n",
        "        # Linear layers with SiLU activation.\n",
        "        self.w1 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(config.hidden_dim, config.dim, bias=False)\n",
        "        self.w3 = nn.Linear(config.dim, config.hidden_dim, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)  # Dropout after the FFN\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFN transformation:  x -> SiLU(xW1) * (xW3) -> (result)W2 -> dropout\n",
        "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_id: int, config: LMConfig):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.dim = config.dim\n",
        "        self.head_dim = config.dim // config.n_heads\n",
        "        self.attention = Attention(config)  # Multi-head attention\n",
        "\n",
        "        self.layer_id = layer_id\n",
        "        # Layer normalization for attention and FFN.\n",
        "        self.attention_norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(config.dim, eps=config.norm_eps)\n",
        "        # Feedforward network.\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, x, pos_cis, past_key_value=None, use_cache=False):\n",
        "        # Attention block with residual connection.\n",
        "        h_attn, past_kv = self.attention(\n",
        "            self.attention_norm(x),  # Normalize input before attention\n",
        "            pos_cis,  # Rotary positional embeddings\n",
        "            past_key_value=past_key_value,  # Pass cached key-value pairs\n",
        "            use_cache=use_cache  # Whether to use caching\n",
        "        )\n",
        "        h = x + h_attn  # Residual connection\n",
        "        # Feedforward block with residual connection.\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))  # Normalize input before FFN\n",
        "        return out, past_kv\n",
        "\n",
        "\n",
        "class TransformerLM(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    The main Transformer language model.\n",
        "    \"\"\"\n",
        "    config_class = LMConfig  # Use LMConfig as the configuration class\n",
        "\n",
        "    def __init__(self, params: LMConfig = None):\n",
        "        self.params = params or LMConfig()  # Use default config if none provided\n",
        "        super().__init__(self.params)  # Initialize PreTrainedModel\n",
        "        self.vocab_size, self.n_layers = params.vocab_size, params.n_layers\n",
        "        # Token embeddings.\n",
        "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "        self.dropout = nn.Dropout(params.dropout)  # Dropout after embeddings\n",
        "        # Transformer blocks.\n",
        "        self.layers = nn.ModuleList([TransformerBlock(l, params) for l in range(self.n_layers)])\n",
        "        # Final layer normalization.\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        # Output layer (maps from hidden states to logits).\n",
        "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
        "        # Tie token embeddings and output weights.\n",
        "        self.tok_embeddings.weight = self.output.weight\n",
        "        # Precompute and register rotary positional embeddings.\n",
        "        self.register_buffer(\"pos_cis\",\n",
        "                             precompute_pos_cis(dim=params.dim // params.n_heads, theta=params.rope_theta),\n",
        "                             persistent=False)\n",
        "        self.OUT = CausalLMOutputWithPast()  # Use CausalLMOutputWithPast for output\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids: Optional[torch.Tensor] = None,\n",
        "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
        "                use_cache: bool = False,\n",
        "                **args):\n",
        "\n",
        "        past_key_values = past_key_values or [None] * len(self.layers)  # Initialize empty cache if None\n",
        "        start_pos = args.get('start_pos', 0)  # Get starting position for sequence generation\n",
        "\n",
        "        h = self.dropout(self.tok_embeddings(input_ids))  # Get token embeddings and apply dropout\n",
        "        pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]  # Get relevant rotary embeddings\n",
        "\n",
        "        past_kvs = []  # Store past key-value pairs for caching\n",
        "        for l, layer in enumerate(self.layers):\n",
        "            h, past_kv = layer(\n",
        "                h, pos_cis,\n",
        "                past_key_value=past_key_values[l],  # Pass cached key-value pairs\n",
        "                use_cache=use_cache\n",
        "            )\n",
        "            past_kvs.append(past_kv)  # Store updated key-value pairs\n",
        "\n",
        "        logits = self.output(self.norm(h))  # Final layer normalization and output projection\n",
        "\n",
        "        # Set output attributes using __setitem__\n",
        "        self.OUT.__setitem__('logits', logits)\n",
        "        self.OUT.__setitem__('past_key_values', past_kvs)\n",
        "        return self.OUT  # Return CausalLMOutputWithPast object\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, input_ids, eos_token_id=2, max_new_tokens=1024, temperature=0.75, top_p=0.90,\n",
        "                 stream=False, rp=1., use_cache=True, pad_token_id=0, **args):\n",
        "        \"\"\"\n",
        "        Generates text from the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Initial input token IDs.\n",
        "            eos_token_id: End-of-sequence token ID.\n",
        "            max_new_tokens: Maximum number of tokens to generate.\n",
        "            temperature: Sampling temperature.\n",
        "            top_p: Top-p (nucleus) sampling probability.\n",
        "            stream: Whether to stream the output (yield tokens one by one).\n",
        "            rp: Repetition penalty.\n",
        "            use_cache: Whether to use key-value caching.\n",
        "            pad_token_id: pad token id.\n",
        "            **args: Additional arguments passed to the forward method.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Generated sequence of token IDs.\n",
        "                         (if stream=False)\n",
        "            Generator[torch.Tensor, None, None]:  Generated tokens.\n",
        "                                        (if stream=True)\n",
        "        \"\"\"\n",
        "        # Stream generation\n",
        "        if stream:\n",
        "            return self._stream(input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args)\n",
        "\n",
        "        # Direct generation (collects all tokens at once)\n",
        "        generated = []\n",
        "        for i in range(input_ids.size(0)):\n",
        "            # remove padding\n",
        "            non_pad = input_ids[i][input_ids[i] != pad_token_id].unsqueeze(0)\n",
        "            # generate tokens\n",
        "            out = self._stream(non_pad, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args)\n",
        "            # collect the generated token one-by-one\n",
        "            tokens_list = [tokens[:, -1:] for tokens in out]\n",
        "            gen = torch.cat(tokens_list, dim=-1) if tokens_list else non_pad\n",
        "            # concat the input and generated tokens together\n",
        "            full_sequence = torch.cat([non_pad, gen], dim=-1)\n",
        "            generated.append(full_sequence)\n",
        "        # find the longest sequence\n",
        "        max_length = max(seq.size(1) for seq in generated)\n",
        "        # padding the sequences\n",
        "        generated = [\n",
        "            torch.cat(\n",
        "                [seq, torch.full((1, max_length - seq.size(1)), pad_token_id, dtype=seq.dtype, device=seq.device)],\n",
        "                dim=-1)\n",
        "            for seq in generated\n",
        "        ]\n",
        "        # concatenate all generated tensors together\n",
        "        return torch.cat(generated, dim=0)\n",
        "\n",
        "    def _stream(self, input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache, **args):\n",
        "        \"\"\"\n",
        "        Helper function for streaming text generation.\n",
        "        \"\"\"\n",
        "        start, first_seq, past_kvs = input_ids.shape[1], True, None\n",
        "        while input_ids.shape[1] < max_new_tokens - 1:\n",
        "            if first_seq or not use_cache:\n",
        "                # For the first sequence or when not using cache, process the entire input sequence.\n",
        "                out, first_seq = self(input_ids, past_key_values=past_kvs, use_cache=use_cache, **args), False\n",
        "            else:\n",
        "                # For subsequent sequences with caching, process only the last token.\n",
        "                out = self(input_ids[:, -1:], past_key_values=past_kvs, use_cache=use_cache,\n",
        "                           start_pos=input_ids.shape[1] - 1, **args)\n",
        "            logits, past_kvs = out.logits[:, -1, :], out.past_key_values  # Get logits and updated cache\n",
        "\n",
        "            # Apply repetition penalty.\n",
        "            logits[:, list(set(input_ids.tolist()[0]))] /= rp\n",
        "\n",
        "            # Apply temperature scaling.\n",
        "            logits /= (temperature + 1e-9)\n",
        "\n",
        "            # Apply top-p (nucleus) sampling.\n",
        "            if top_p is not None and top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "                sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = False\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits[indices_to_remove] = -float('Inf')  # Set probabilities to -inf for filtered tokens\n",
        "\n",
        "            # Sample the next token.\n",
        "            input_ids_next = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)  # Append the new token to the sequence\n",
        "            yield input_ids[:, start:]  # Yield the generated tokens (excluding the initial input)\n",
        "\n",
        "            # Break if EOS token is generated.\n",
        "            if input_ids_next.item() == eos_token_id:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTransformer(PreTrainedModel):\n",
        "    config_class = dict  # We'll use a dictionary for configuration\n",
        "\n",
        "    def __init__(self, config: dict):\n",
        "        super().__init__(config)  # Initialize PreTrainedModel\n",
        "        self.config = config # store config\n",
        "        self.vocab_size, self.n_layers = config[\"vocab_size\"], config[\"n_layers\"]\n",
        "\n",
        "        # Token embeddings\n",
        "        self.tok_embeddings = nn.Embedding(config[\"vocab_size\"], config[\"dim\"])\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([TransformerBlock(l, config) for l in range(self.n_layers)])\n",
        "\n",
        "        # Final normalization layer\n",
        "        self.norm = RMSNorm(config[\"dim\"], eps=config[\"norm_eps\"])\n",
        "\n",
        "        # Output layer (linear projection to vocabulary size)\n",
        "        self.output = nn.Linear(config[\"dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "        # Tie the weights of the embedding and output layers\n",
        "        self.tok_embeddings.weight = self.output.weight\n",
        "\n",
        "        # Precompute rotary positional embeddings\n",
        "        self.register_buffer(\n",
        "            \"pos_cis\",\n",
        "            precompute_pos_cis(dim=config[\"dim\"] // config[\"n_heads\"], theta=config[\"rope_theta\"]),\n",
        "            persistent=False,\n",
        "        )\n",
        "        self.OUT = CausalLMOutputWithPast()\n",
        "\n",
        "    def forward(self,\n",
        "                input_ids: Optional[torch.Tensor] = None,\n",
        "                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,\n",
        "                use_cache: bool = False,\n",
        "                **args):\n",
        "\n",
        "        past_key_values = past_key_values or [None] * len(self.layers)\n",
        "        start_pos = args.get('start_pos', 0)\n",
        "\n",
        "        # Apply dropout to the embeddings\n",
        "        h = self.dropout(self.tok_embeddings(input_ids))\n",
        "\n",
        "        # Get the appropriate positional embeddings for the current sequence length\n",
        "        pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]\n",
        "\n",
        "        # Iterate through the Transformer blocks\n",
        "        past_kvs = []\n",
        "        for l, layer in enumerate(self.layers):\n",
        "            h, past_kv = layer(\n",
        "                h, pos_cis,\n",
        "                past_key_value=past_key_values[l],\n",
        "                use_cache=use_cache\n",
        "            )\n",
        "            past_kvs.append(past_kv)\n",
        "\n",
        "        # Apply the final normalization and output layer\n",
        "        logits = self.output(self.norm(h))\n",
        "        # aux_loss = sum(l.feed_forward.aux_loss for l in self.layers if isinstance(l.feed_forward, MOEFeedForward)) #we are not using moe for now.\n",
        "\n",
        "        self.OUT.__setitem__('logits', logits)\n",
        "        self.OUT.__setitem__('aux_loss', 0)  # we set 0 because we are not using moe for now\n",
        "        self.OUT.__setitem__('past_key_values', past_kvs)\n",
        "        return self.OUT"
      ],
      "metadata": {
        "id": "HcGaeHttuPod"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Bienvenue dans Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ab559df54652bb95e07787d59a59d160dc53acec48e26b004f7b380e6f234b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}